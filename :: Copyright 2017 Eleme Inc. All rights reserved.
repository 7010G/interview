// Copyright 2017 Eleme Inc. All rights reserved.

package controller

import (
	"errors"
	"fmt"
	"io"
	"net/http"
	"os"
	"sort"
	"strings"
	"sync"
	"time"

	"git.elenet.me/MaxQ/qboard/broker"
	"git.elenet.me/MaxQ/qboard/logger"
	"git.elenet.me/MaxQ/qboard/storage/mongo"

	"github.com/abunchofnothing/goguid"
	"github.com/eleme/log"
	"github.com/gorilla/mux"
	"github.com/streadway/amqp"

	"gopkg.in/mgo.v2/bson"
)

// const ...
const (
	stateReady       = "ready"
	stateRunning     = "running"
	stateDone        = "done"
	stateInterrupted = "interrupted"

	exPrefix    = "qboard-ex-"
	shardPrefix = "sharding:"
)

var (
	runningTasks = make(map[string]*Task)
	lock         sync.RWMutex
)

// TransferOption ...
type TransferOption struct {
	SrcReg   string   `json:"src_reg"`
	DestReg  string   `json:"dest_reg"`
	VHosts   VHosts   `json:"vhosts"`
	SrcAddr  *address `json:"src_addr"`
	DestAddr *address `json:"dest_addr"`
	SrcLink  string   `json:"src_link"` // link user for web jump
	DestLink string   `json:"dest_link"`
	IsDirect bool     `json:"is_direct"` // direct transfer Task or not
	TaskParams
}

// TaskParams ...
type TaskParams struct {
	ConsumerNum   int `json:"consumer_num"`   // 每个queue的消费者数
	ConnectionNum int `json:"connection_num"` // 每个queue的连接数
	PrefetchCount int `json:"prefetch_count"`
}

type brokerWrapper struct {
	src  *broker.Broker
	dest *broker.Broker
}

// Task ...
type Task struct {
	log.SimpleLogger             `json:"-" bson:"-"`
	sync.Mutex                   `json:"-" bson:"-"`
	GUID         string          `json:"id" bson:"id"`
	CreateAt     time.Time       `json:"create_at" bson:"create_at"`
	UpdateAt     time.Time       `json:"update_at" bson:"update_at"`
	State        string          `json:"state" bson:"state"`
	Operator     string          `json:"operator" bson:"operator"`
	TransferOpts *TransferOption `json:"transfer_options" bson:"transfer_options"`

	vhostQueues map[string]Queues
	vhostBroker map[string][]*brokerWrapper
	logFile     *os.File `bson:"-"`

	RunningNode string `json:"running_node"` // 使用mongo后，节点共享task数据，如果taks被打断，更新qboard时多节点都会拉起数据，所以需要保证
	IsThisNode  bool   `json:"is_this_node"` // 该任务的创建节点是否是本节点
}

type srcPayload struct {
	SrcList []string `json:"src_list"`
}

func newTask(opts *TransferOption, operator string) (*Task, error) {
	if opts == nil {
		return nil, nil
	}
	now := time.Now()
	id := guid.Generate()

	logName := "Task." + id[0:4]

	_, err := os.Stat(cfg.TasksLog)
	if os.IsNotExist(err) {
		if err := os.MkdirAll(cfg.TasksLog, 0755); err != nil {
			return nil, err
		}
	}
	logFile, err := os.Create(cfg.TasksLog + "/" + id)
	if err != nil {
		return nil, err
	}

	t := &Task{
		SimpleLogger: logger.NewWithWriter(logName, logFile),
		GUID:         id,
		CreateAt:     now,
		UpdateAt:     now,
		State:        stateReady,
		Operator:     operator,
		TransferOpts: opts,

		vhostBroker: make(map[string][]*brokerWrapper),
		vhostQueues: make(map[string]Queues),
		logFile:     logFile,
	}
	return t, nil
}

func rebuildTask(t *Task) error {
	// try to get the newest address from huskar
	if !t.TransferOpts.IsDirect {
		if t.TransferOpts.SrcReg != "" && t.TransferOpts.DestReg != "" {
			srcAddr, err := getAddressFromHuskar(t.TransferOpts.SrcReg)
			if err == nil {
				t.TransferOpts.SrcAddr = srcAddr
			} else {
				l.Error(err)
			}

			destAddr, err := getAddressFromHuskar(t.TransferOpts.DestReg)
			if err == nil {
				t.TransferOpts.DestAddr = destAddr
			} else {
				l.Error(err)
			}
		}
	}

	if t.TransferOpts.SrcAddr == nil || t.TransferOpts.DestAddr == nil {
		return fmt.Errorf("no source or destination addr found")
	}

	if t.vhostQueues == nil {
		t.vhostQueues = make(map[string]Queues)
	}

	if t.vhostBroker == nil {
		t.vhostBroker = make(map[string][]*brokerWrapper)
	}

	_, err := os.Stat(cfg.TasksLog)
	if os.IsNotExist(err) {
		if err := os.MkdirAll(cfg.TasksLog, 0755); err != nil {
			return err
		}
	}

	var logFile *os.File

	logFile, err = os.OpenFile(cfg.TasksLog+"/"+t.GUID, os.O_RDWR|os.O_APPEND, 0666)
	if err != nil {
		if os.IsNotExist(err) {
			logFile, err = os.Create(cfg.TasksLog + "/" + t.GUID)
			if err != nil {
				return err
			}
		} else {
			return err
		}
	}
	t.logFile = logFile

	logName := "Task." + t.GUID[0:4]
	t.SimpleLogger = logger.NewWithWriter(logName, logFile)

	return nil
}

func (t *Task) needRecovery() (yes bool) {
	switch t.State {
	case stateRunning, stateInterrupted: // stateRunning: tasks exit with SIGKILL, SIGSTOP
		return true
	}
	return
}

// belongToNode check is this task belong to this node.
// xg 2 node, wg 2 node;
// such as if  there are tasks running on one node in xg, update two node will interrupt this task
// if no check in node, two node will both resume this task.
// check task node can confirm only one node will run this task.
func (t *Task) belongToNode() bool {
	if cfg.HostName == "" || t.RunningNode == "" {
		return false
	}

	return t.RunningNode == cfg.HostName
}

func (t *Task) start() error {
	t.Lock()
	defer t.Unlock()
	if t.State == stateRunning {
		return fmt.Errorf("task state is already running")
	}

	go t.run()
	t.State = stateRunning

	lock.Lock()
	defer lock.Unlock()
	runningTasks[t.GUID] = t

	err := setTask(t)
	if err != nil {
		go t.stop()
		return fmt.Errorf("save Task to mongo faiel: %s, task will be stop after a little seconds", err)
	}

	return nil
}

// restart restart the running before or interrupted Task
func (t *Task) restart() {
	t.Lock()
	defer t.Unlock()
	if t.State == stateRunning || t.State == stateInterrupted {
		go t.run()
		t.State = stateRunning

		lock.Lock()
		defer lock.Unlock()

		runningTasks[t.GUID] = t

		err := setTask(t)
		if err != nil {
			l.Errorf("task[%s] restart save to mongo failed: %s", err)
		}
	}
	return
}

func (t *Task) reOpen() error {
	t.Lock()
	defer t.Unlock()
	if t.State == stateDone {
		go t.run()
		t.State = stateRunning

		lock.Lock()
		defer lock.Unlock()
		runningTasks[t.GUID] = t

		err := setTask(t)
		if err != nil {
			return fmt.Errorf("save task to mongo failed: %s", err)
		}
	}

	return nil
}

func (t *Task) run() {
	wg := &sync.WaitGroup{}
	for _, vhost := range t.TransferOpts.VHosts {
		queues := make(Queues, 0)
		for _, queue := range vhost.Queues {
			if queue.Name == "aliveness-test" || queue.AutoDelete {
				continue
			}
			queues = append(queues, queue)
		}
		if len(queues) == 0 {
			continue
		}

		t.vhostQueues[vhost.Name] = queues

		//t.handleForVHost(wg, vhost, queues, &t.TransferOpts.TaskParams)
		t.newHandleForVHost(wg, vhost, queues, &t.TransferOpts.TaskParams)
	}
	wg.Wait()
}

// default five queue one connection
// option xx queue xx connection
func (t *Task) buildConsumer(wg *sync.WaitGroup, srcBK, destBK *broker.Broker, declareChan *amqp.Channel, vhostName string, queue *Queue) {
	queueName := queue.Name
	targetEx := exPrefix + queueName

	if strings.HasPrefix(queueName, shardPrefix) {
		temp := strings.TrimLeft(queueName, shardPrefix)
		if index := strings.Index(temp, "-"+queue.Node); index > 0 {
			originQueue := temp[:index]
			targetEx = exPrefix + originQueue
		}
	}
	targetEx = targetEx + "-" + strings.Split(t.GUID, "-")[4]

	rc := srcBK.RetryConsumer(queueName) // one consumer use one channel
	rp := destBK.RetryProducer()         // one producer use one channel of destBk
	if err := declareChan.ExchangeDeclare(targetEx, amqp.ExchangeFanout, true, false, false, false, nil); err != nil {
		t.Errorf("[%s] declare exchange: %s got error: %s", vhostName, targetEx, err)
		return
	}
	t.Infof("[%s] declare exchange: %s succeeded", vhostName, targetEx)

	if err := declareChan.QueueBind(queueName, "", targetEx, false, nil); err != nil {
		t.Errorf("[%s] bind queue: %s to exchange: %s got error: %s", vhostName, queueName, targetEx, err)
		return
	}
	t.Infof("[%s] bind queue: %s to exchange: %s succeeded", vhostName, queueName, targetEx)
	wg.Add(1)

	go t.consumeLoop(wg, vhostName, queueName, targetEx, rc, rp)
}

func (t *Task) handleForVHost(wg *sync.WaitGroup, vhost *VHost, queues Queues, parms *TaskParams) {
	srcURI := buildAMQPURI(t.TransferOpts.SrcAddr, vhost.Name)
	t.Infof("[%s] source AMQP: %s", vhost.Name, srcURI.String())
	destURI := buildAMQPURI(t.TransferOpts.DestAddr, vhost.Name)
	t.Infof("[%s] dest AMQP: %s", vhost.Name, destURI.String())

	// if Task param is not default
	if parms.ConnectionNum > 0 {
		destBK := broker.New(broker.Option{URI: destURI}, t.SimpleLogger) // all producer of queue in this vhost use one connection
		for _, queue := range queues {

			srcBKs := make(chan *broker.Broker, parms.ConnectionNum)
			for i := 0; i < parms.ConnectionNum; i++ {
				// new src broker
				srcBK := broker.New(broker.Option{URI: srcURI, PrefetchCount: parms.PrefetchCount}, t.SimpleLogger)

				bkWrapper := &brokerWrapper{
					src:  srcBK,
					dest: destBK,
				}
				// save broker
				t.vhostBroker[vhost.Name] = append(t.vhostBroker[vhost.Name], bkWrapper)

				srcBKs <- srcBK
			}

			for i := 0; i < parms.ConsumerNum; i++ {
				srcBK := <-srcBKs
				t.doDeclareAndBinding(wg, srcBK, destBK, queue, vhost.Name)
				srcBKs <- srcBK
			}
		}

	} else {
		srcBK := broker.New(broker.Option{URI: srcURI, PrefetchCount: parms.PrefetchCount}, t.SimpleLogger)
		destBK := broker.New(broker.Option{URI: destURI}, t.SimpleLogger) // all producer of queue in this vhost use one connection
		bkWrapper := &brokerWrapper{
			src:  srcBK,
			dest: destBK,
		}
		t.vhostBroker[vhost.Name] = append(t.vhostBroker[vhost.Name], bkWrapper)

		for _, queue := range queues {
			t.doDeclareAndBinding(wg, srcBK, destBK, queue, vhost.Name)
		}
	}
}

func (t *Task) newHandleForVHost(wg *sync.WaitGroup, vhost *VHost, queues Queues, parms *TaskParams) {
	srcURI := buildAMQPURI(t.TransferOpts.SrcAddr, vhost.Name)
	t.Infof("[%s] source AMQP: %s", vhost.Name, srcURI.String())
	destURI := buildAMQPURI(t.TransferOpts.DestAddr, vhost.Name)
	t.Infof("[%s] dest AMQP: %s", vhost.Name, destURI.String())

	// all param in parms refer to 0 while switch cluster
	if parms.ConnectionNum == 0 && parms.ConsumerNum == 0{
		// set default value for TaskParams
		parms = &TaskParams{
			ConsumerNum:   1,
			ConnectionNum: 1,
			PrefetchCount: 1000,
		}
	}

	destBK := broker.New(broker.Option{URI: destURI}, t.SimpleLogger) // all producer of queue in this vhost use one connection
	for _, queue := range queues {

		srcBKs := make(chan *broker.Broker, parms.ConnectionNum)
		for i := 0; i < parms.ConnectionNum; i++ {
			// new src broker
			srcBK := broker.New(broker.Option{URI: srcURI, PrefetchCount: parms.PrefetchCount}, t.SimpleLogger)

			bkWrapper := &brokerWrapper{
				src:  srcBK,
				dest: destBK,
			}
			// save broker
			t.vhostBroker[vhost.Name] = append(t.vhostBroker[vhost.Name], bkWrapper)

			srcBKs <- srcBK
		}

		for i := 0; i < parms.ConsumerNum; i++ {
			srcBK := <-srcBKs
			t.doDeclareAndBinding(wg, srcBK, destBK, queue, vhost.Name)
			srcBKs <- srcBK
		}
	}
}

// doDeclareAndBinding each declare and binding for one queue use different channel, because if err occur in binding, channel will be closed.
func (t *Task) doDeclareAndBinding(wg *sync.WaitGroup, srcBK *broker.Broker, destBK *broker.Broker, queue *Queue, vhostName string) {
	pubChan, err := destBK.GetPubChannel()
	if err != nil {
		t.Errorf("[%s] get publish channel got error: %s", vhostName, err)
		return
	}
	defer pubChan.Close()

	t.buildConsumer(wg, srcBK, destBK, pubChan, vhostName, queue)
}

func (t *Task) consumeLoop(wg *sync.WaitGroup, vhost string, queue string, exchange string, rc *broker.RetryConsumer, rp *broker.RetryProducer) {
	defer wg.Done()

	var consumed, acked, ackErr, pubErr, nacked, nackErr = 0, 0, 0, 0, 0, 0
	for {
		d := rc.Consume()
		if d == nil {
			break
		}
		consumed++

		if err := rp.Publish(exchange, d.RoutingKey, newPublishing(d)); err != nil {
			t.Errorf("[%s] publish to exchange: %s with key: %s got error: %s", vhost, exchange, d.RoutingKey, err)
			pubErr++
			if err := d.Nack(false, true); err != nil {
				t.Errorf("[%s] nack for queue: %s got error: %s", vhost, queue, err)
				nackErr++
			}
			nacked++
			continue
		}
		if err := d.Ack(false); err != nil {
			t.Errorf("[%s] ack for queue: %s got error: %s", vhost, queue, err)
			ackErr++
		}
		acked++
	}
	t.Infof("[%s] queue: %s transfer messages, consumed: %d, acked: %d, ack error: %d, publish error: %d, nacked: %d, nack error: %d",
		vhost, queue, consumed, acked, ackErr, pubErr, nacked, nackErr)
}

func newPublishing(d *amqp.Delivery) *amqp.Publishing {
	return &amqp.Publishing{
		Headers:         d.Headers,
		ContentType:     d.ContentType,
		ContentEncoding: d.ContentEncoding,
		DeliveryMode:    d.DeliveryMode,
		Priority:        d.Priority,
		CorrelationId:   d.CorrelationId,
		ReplyTo:         d.ReplyTo,
		Expiration:      d.Expiration,
		MessageId:       d.MessageId,
		Timestamp:       d.Timestamp,
		Type:            d.Type,
		UserId:          d.UserId,
		AppId:           d.AppId,
		Body:            d.Body,
	}
}

func (t *Task) interrupt() {
	t.Lock()
	defer t.Unlock()

	if t.State == stateDone || t.State == stateInterrupted {
		return
	}

	defer func() {
		t.State = stateInterrupted

		lock.Lock()
		defer lock.Unlock()
		delete(runningTasks, t.GUID)

		err := setTask(t)
		if err != nil {
			l.Errorf("save task[%s] to mongo error: %s", t.GUID, err)
		}

		if t.logFile != nil {
			t.logFile.Close()
		}
	}()

	sweepResource(t)
	return
}

func (t *Task) setStop() {
	for _, bks := range t.vhostBroker {
		for _, bk := range bks {
			if bk.src != nil {
				bk.src.SetClosing()
			}
			if bk.dest != nil {
				bk.dest.SetClosing()
			}
		}
	}
}

func (t *Task) stop() error {
	t.Lock()
	defer t.Unlock()
	t.setStop()

	if t.State == stateDone {
		return nil
	}

	defer func() error {
		t.State = stateDone

		lock.Lock()
		defer lock.Unlock()
		delete(runningTasks, t.GUID)

		err := setTask(t)
		if err != nil {
			return fmt.Errorf("stop task success, but update task state to mongo error: %s", err)
		}

		if t.logFile != nil {
			t.logFile.Close()
		}

		return nil
	}()

	sweepResource(t)

	return nil
}

func sweepResource(t *Task) {
	if t.State != stateRunning {
		return
	}

	for vhostName, queues := range t.vhostQueues {
		bks, ok := t.vhostBroker[vhostName]
		if ok {
			for _, bk := range bks {

				pubChan, err := bk.dest.GetPubChannel()
				if err != nil {
					t.Errorf("[%s] get publish channel got error: %s", vhostName, err)
					continue
				}
				for _, queue := range queues {
					if strings.HasPrefix(queue.Name, shardPrefix) {
						continue
					}
					targetEx := exPrefix + queue.Name + "-" + strings.Split(t.GUID, "-")[4]
					if err := pubChan.ExchangeDelete(targetEx, false, false); err != nil {
						t.Errorf("[%s] delete exchange: %s got error: %s", vhostName, targetEx, err)
					} else {
						t.Infof("[%s] exchange: %s deleted", vhostName, targetEx)
					}
				}
				pubChan.Close()
			}
		}
	}

	for vhost, bks := range t.vhostBroker {
		for _, bk := range bks {
			if bk.src != nil {
				if err := bk.src.Close(); err != nil {
					t.Errorf("[%s] close source broker got error: %s", vhost, err)
				}
			}
			if bk.dest != nil {
				if err := bk.dest.Close(); err != nil {
					t.Errorf("[%s] close dest broker got error: %s", vhost, err)
				}
			}
		}
	}
	return
}

func buildTransferOptions(payload *transferPayload, ft *VHost) (*TransferOption, error) {
	var err error
	var srcAddr *address
	var destAddr *address

	// cluster switch task
	if payload.IDC != "" && payload.Cluster != "" {
		// switch link transfer Task
		// idc and cluster item is only used for distinguishing switch Task and transfer page Task
		node, err := getRandomNode(payload.IDC, payload.Cluster)
		if err != nil {
			return nil, fmt.Errorf("get cluster node error: %s", err)
		}
		srcAddr = &address{
			IP:          node.IP,
			Port:        node.Port,
			MonitorPort: node.MonitorPort,
		}
	} else {
		// if idc and cluster is null, it means this Task created by transfer page
		if payload.DirectSrc != "" {
			ip, port, monitorPort, err := splitTaskDirectLink(payload.DirectSrc)
			if err != nil {
				return nil, err
			}

			srcAddr = &address{ip, port, monitorPort}
		} else {
			// direct transfer by reg
			//TODO: can not get monitor port from huskar, should reg monitor port to huskar.
			srcAddr, err = getAddressByReg(payload.SrcReg)
			if err != nil {
				return nil, err
			}
		}
	}

	if payload.DirectDest != "" {
		ip, port, monitorPort, err := splitTaskDirectLink(payload.DirectDest)
		if err != nil {
			return nil, err
		}
		destAddr = &address{ip, port, monitorPort}

	} else {
		destAddr, err = getAddressByReg(payload.DestReg)
		if err != nil {
			return nil, err
		}
	}

	if payload.SrcLink == "" {
		payload.SrcLink, err = getRegAddress(payload.SrcReg)
		if err != nil {
			l.Errorf("get src reg domain link error: %s", err)
		}
	}

	if payload.DestLink == "" {
		payload.DestLink, err = getRegAddress(payload.DestReg)
		if err != nil {
			l.Errorf("get dest reg domain link error: %s", err)
		}
	}

	srcEndpoint := fmt.Sprintf("http://%s:%d", srcAddr.IP, srcAddr.MonitorPort)
	srcVHosts, err := ListVHosts(srcEndpoint, UserAdmin, cfg.AdminPass)
	if err != nil {
		return nil, err
	}

	destEndpoint := fmt.Sprintf("http://%s:%d", destAddr.IP, destAddr.MonitorPort)
	destVHosts, err := ListVHosts(destEndpoint, UserAdmin, cfg.AdminPass)
	if err != nil {
		return nil, err
	}
	destVHostsMap := vhostsToMap(destVHosts)

	bothVHosts := make(VHosts, 0)
	for i := range srcVHosts {
		vhost := srcVHosts[i]
		if _, ok := destVHostsMap[vhost.Name]; ok {
			// keep specified vhost
			if ft != nil && vhost.Name != ft.Name {
				continue
			}
			bothVHosts = append(bothVHosts, vhost)
		}
	}
	opts := &TransferOption{
		SrcReg:     payload.SrcReg,
		DestReg:    payload.DestReg,
		VHosts:     bothVHosts,
		SrcAddr:    srcAddr,
		DestAddr:   destAddr,
		SrcLink:    payload.SrcLink,
		DestLink:   payload.DestLink,
		TaskParams: payload.Params,
	}

	opts.IsDirect = payload.DirectSrc != "" || payload.DirectDest != ""

	if payload.DirectSrc != "" {
		opts.SrcReg = payload.DirectSrc
	}
	if payload.DirectDest != "" {
		opts.DestReg = payload.DirectDest
	}

	errs := []error{}
	for _, vhost := range bothVHosts {
		if ft != nil && vhost.Name != ft.Name {
			continue
		}

		srcQueues, err := ListQueues(srcEndpoint, vhost.Name, UserAdmin, cfg.AdminPass)
		if err != nil {
			// some vhost may have not user, so using admin will be unauthorized
			if err == ErrUnauthorized {
				continue
			}
			errs = append(errs, err)
			continue
		}

		destQueues, err := ListQueues(destEndpoint, vhost.Name, UserAdmin, cfg.AdminPass)
		if err != nil {
			if err == ErrUnauthorized {
				continue
			}
			errs = append(errs, err)
			continue
		}
		destQueuesMap := queuesToMap(destQueues)

		bothQueues := make(Queues, 0)
		for i := range srcQueues {
			queue := srcQueues[i]
			if _, ok := destQueuesMap[queue.Name]; ok {
				// keep specified queues
				if ft != nil && len(ft.Queues) > 0 {
					if _, exists := ft.queuesMap[queue.Name]; !exists {
						continue
					}
				}
				bothQueues = append(bothQueues, queue)
			}
		}
		vhost.Queues = bothQueues
	}

	if len(errs) > 0 {
		errString := ""
		for _, e := range errs {
			errString += fmt.Sprintf("%s", e)
		}
		return opts, errors.New(errString)
	}
	return opts, nil
}

type taskList []*Task

func (ts taskList) Len() int      { return len(ts) }
func (ts taskList) Swap(i, j int) { ts[i], ts[j] = ts[j], ts[i] }
func (ts taskList) Less(i, j int) bool {
	return ts[i].CreateAt.After(ts[j].CreateAt)
}

func saveTaskToMongo(t *Task) error {
	t.RunningNode = cfg.HostName

	err := mongo.SaveOrUpdate(mongo.TaskMongoTable, bson.M{"id": t.GUID}, &t)
	if err != nil {
		return err
	}

	return nil
}

func setTask(t *Task) error {
	if t == nil {
		return nil
	}
	t.UpdateAt = time.Now()

	return saveTaskToMongo(t)
}

func getTask(id string) (*Task, error) {
	session, query, err := mongo.Get(mongo.TaskMongoTable, bson.M{"id": id})
	if err != nil {
		return nil, err
	}
	defer session.Close()

	task := &Task{}
	err = query.One(task)
	if err != nil {
		return nil, err
	}

	return task, nil
}

func getTasks(nameList *srcPayload) (taskList, error) {
	result := make(taskList, 0)
	tasks, err := getTaskList()
	if err != nil {
		return tasks, err
	}
	for _, t := range tasks {

		if isExist(t.TransferOpts.SrcReg, nameList.SrcList) || isExist(t.TransferOpts.DestReg, nameList.SrcList) {
			result = append(result, t)
		}
	}

	return result, nil
}

func getTaskList() (taskList, error) {
	result := make(taskList, 0)

	session, query, err := mongo.Get(mongo.TaskMongoTable, nil)
	if err != nil {
		return nil, err
	}
	defer session.Close()

	err = query.All(&result)
	if err != nil {
		return nil, err
	}

	for _, task := range result {
		task.IsThisNode = IsThisNode(task.RunningNode)
	}

	sort.Sort(result)

	return result, nil
}

func deleteTask(id string) error {
	logFile := cfg.TasksLog + "/" + id
	if _, err := os.Stat(logFile); err == nil {
		os.Remove(logFile)
	}

	return deleteMongoTask(id)
}

func deleteMongoTask(id string) error {
	return mongo.Delete(mongo.TaskMongoTable, bson.M{"id": id})
}

func multiDeleteTask(ids []string) error {
	for _, id := range ids {
		err := deleteTask(id)
		if err != nil {
			return err
		}
	}

	return nil
}

// CreateTask create a Task.
// POST "/api/tasks"
func CreateTask(w http.ResponseWriter, r *http.Request) {
	payload := &transferPayload{}
	if err := ReadJSON(r, payload); err != nil {
		ResponseError(w, NewErrBadRequest(err))
		return
	}
	if payload.Params.ConnectionNum > payload.Params.ConsumerNum {
		ResponseError(w, NewErrBadRequest(fmt.Errorf("connection num over consumer num")))
		return
	}
	operator := "unknown"
	if user := getFrontUser(r); user != nil {
		operator = user.Name
	}

	err := createTask(operator, payload)
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}

	ResponseJSONOK(w, "ok")
}

func createTask(operator string, payload *transferPayload) error {
	// init Task config
	opts, err := checkTransfer(payload)
	if err != nil {
		return err
	}

	t, err := newTask(opts, operator)
	if err != nil {
		return err
	}

	err = t.start()
	if err != nil {
		return err
	}

	return nil
}

// GetTask get a Task.
// GET "/api/tasks/{id}"
func GetTask(w http.ResponseWriter, r *http.Request) {
	id := mux.Vars(r)["id"]
	t, err := getTask(id)
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	ResponseJSONOK(w, t)
}

// GetTasks get tasks for one source
// POST "/api/tasks/cluster"
func GetTasks(w http.ResponseWriter, r *http.Request) {
	payLoad := &srcPayload{}
	if err := ReadJSON(r, payLoad); err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}

	t, err := getTasks(payLoad)
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	ResponseJSONOK(w, t)
}

// StopTask stop a running Task.
// PUT "/api/tasks/{id}/stop"
func StopTask(w http.ResponseWriter, r *http.Request) {
	id := mux.Vars(r)["id"]
	t, ok := runningTasks[id]
	if !ok {
		ResponseError(w, NewErrBadRequest(fmt.Errorf("Task: %s not running", id)))
		return
	}
	err := t.stop()
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	ResponseJSONOK(w, "ok")
}

// ReOpenTask re-open a Task which status was done.
// POST "/api/tasks/{id}/reopen"
func ReOpenTask(w http.ResponseWriter, r *http.Request) {
	id := mux.Vars(r)["id"]
	task, err := getTask(id)
	if err != nil {
		ResponseError(w, NewErrInternalError(fmt.Errorf("get task from mongo error: %s", err)))
		return
	}

	if err := rebuildTask(task); err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}

	err = task.reOpen()
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	ResponseJSONOK(w, "ok")
}

// DeleteTask delete a Task.
// DELETE "/api/tasks/{id}"
func DeleteTask(w http.ResponseWriter, r *http.Request) {
	id := mux.Vars(r)["id"]

	if err := deleteTask(id); err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	logFile := cfg.TasksLog + "/" + id
	if _, err := os.Stat(logFile); err == nil {
		os.Remove(logFile)
	}
	ResponseJSONOK(w, "ok")
}

// MultiDeleteTask delete a Task.
// DELETE "/api/tasks/ids/{ids}"
func MultiDeleteTask(w http.ResponseWriter, r *http.Request) {
	ids := mux.Vars(r)["ids"]
	multiID := strings.Split(ids, ",")

	if err := multiDeleteTask(multiID); err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}

	ResponseJSONOK(w, "ok")
}

// GetAllTasks get all tasks.
// GET "/api/tasks"
func GetAllTasks(w http.ResponseWriter, r *http.Request) {
	ts, err := getTaskList()
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}

	ResponseJSONOK(w, ts)
}

// TaskLog return the log of Task.
// GET /api/tasks/{id}/log
func TaskLog(w http.ResponseWriter, r *http.Request) {
	id := mux.Vars(r)["id"]
	f, err := os.Open(cfg.TasksLog + "/" + id)
	if err != nil {
		ResponseError(w, NewErrInternalError(err))
		return
	}
	defer f.Close()
	io.Copy(w, f)
}

// StopRunningTasks will stop all running tasks.
func StopRunningTasks() {
	lock.RLock()
	ts := make(map[string]*Task)
	for id, t := range runningTasks {
		ts[id] = t
	}
	lock.RUnlock()

	for _, t := range ts {
		t.stop()
	}
}

// InterruptRunningTasks interrupt all running tasks.
// Under normal circumstances, we should use the StopRunningTasks method,
// but if not manually trigger to stop the tasks, such as the program abnormal exit, restart, republish, etc.,
// we should call this method to recovery the interrupted tasks after the program is restored.
func InterruptRunningTasks() {
	lock.RLock()
	ts := make(map[string]*Task)
	for id, t := range runningTasks {
		ts[id] = t
	}
	lock.RUnlock()

	for _, t := range ts {
		t.interrupt()
	}
}

// resumeInterruptedTasks resuming the interrupted tasks.
func resumeInterruptedTasks() error {
	tasks, err := getTaskList()
	if err != nil {
		return err
	}

	wg := &sync.WaitGroup{}
	for _, t := range tasks {
		wg.Add(1)
		go recoveryTask(t, wg)
	}

	wg.Wait()
	return nil
}

func recoveryTask(t *Task, wg *sync.WaitGroup) {
	if t.needRecovery() && t.belongToNode() {
		l.Infof("resuming interrupted Task: %s", t.GUID)
		if err := rebuildTask(t); err != nil {
			l.Error("fail to rebuilt Task, error: %v", err)
			wg.Done()
			return
		}

		t.restart()
	}

	wg.Done()
}
